{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Libraries**"
      ],
      "metadata": {
        "id": "Bf5SdVFTCLTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import string\n",
        "import gdown\n",
        "import os\n",
        "import re"
      ],
      "metadata": {
        "id": "veHdCTQ0UFPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Downloading File**"
      ],
      "metadata": {
        "id": "L_q5hXFPCHqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_id = 'FILE_ID'\n",
        "url = f'https://drive.google.com/uc?export=download&id={file_id}'\n",
        "gdown.download(url, quiet=False)"
      ],
      "metadata": {
        "id": "pYVlyypeCCsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_json(\"data 2.json\")"
      ],
      "metadata": {
        "id": "UrilTGYyUIGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing**"
      ],
      "metadata": {
        "id": "TfC4RWa1CPAH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dividing into parts**"
      ],
      "metadata": {
        "id": "ekHyDHDiCUf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "part1_df = df[df.model == \"api.part1question\"]\n",
        "part2_df = df[df.model == \"api.part2question\"]\n",
        "part3_df = df[df.model == \"api.part3question\"]\n",
        "\n",
        "part1_answers_df = df[df.model == \"school_api.schoolpart1result\"]\n",
        "part2_answers_df = df[df.model == \"school_api.schoolpart2result\"]\n",
        "part3_answers_df = df[df.model == \"school_api.schoolpart3result\"]\n",
        "\n",
        "parsed_df = df[df.model == \"school_api.parsedsession\"]"
      ],
      "metadata": {
        "id": "RxjjEALtUJO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Splitting important parts**"
      ],
      "metadata": {
        "id": "Hon1qdrKCY3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def splitter(df):\n",
        "    df_expanded = pd.concat([df.drop(columns=['fields']), df['fields'].apply(pd.Series)], axis=1)\n",
        "    df_expanded = df_expanded[['pk', 'question_txt']]\n",
        "    df_expanded = df_expanded.reset_index(drop=True)\n",
        "\n",
        "    return df_expanded"
      ],
      "metadata": {
        "id": "MGzIX7SSUK2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def splitter_answers(df):\n",
        "    df_expanded = pd.concat([df.drop(columns=['fields']), df['fields'].apply(pd.Series)], axis=1)\n",
        "    df_expanded = df_expanded[[\"session\", 'answer',  'question']]\n",
        "    df_expanded = df_expanded.dropna(subset=['answer'])\n",
        "    df_expanded = df_expanded.reset_index(drop=True)\n",
        "\n",
        "    return df_expanded"
      ],
      "metadata": {
        "id": "7vc4WcawUMdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def splitter_parsed(df):\n",
        "    df_expanded = pd.concat([df.drop(columns=['fields']), df['fields'].apply(pd.Series)], axis=1)\n",
        "    df_expanded = df_expanded[['session', 'raw_json', \"parsed_json\", \"feedback\", \"band_score\"]]\n",
        "    df_expanded = df_expanded.reset_index(drop=True)\n",
        "\n",
        "    return df_expanded"
      ],
      "metadata": {
        "id": "ySXLkP_1UOJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_part1_ques_df = splitter(part1_df)\n",
        "df_part2_ques_df = splitter(part2_df)\n",
        "df_part3_ques_df = splitter(part3_df)\n",
        "\n",
        "df_part1_ans_df = splitter_answers(part1_answers_df)\n",
        "df_part2_ans_df = splitter_answers(part2_answers_df)\n",
        "df_part3_ans_df = splitter_answers(part3_answers_df)\n",
        "\n",
        "df_parsed = splitter_parsed(parsed_df)"
      ],
      "metadata": {
        "id": "9fdsKLIJUPr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Merging Datas**"
      ],
      "metadata": {
        "id": "aqKrB6BRCdCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merged_df(df1, df2):\n",
        "    merged_df = pd.merge(df1, df2, left_on='pk', right_on='question', how='inner')\n",
        "\n",
        "    return merged_df\n",
        "\n",
        "merged_df_1 = merged_df(df_part1_ques_df, df_part1_ans_df)\n",
        "merged_df_2 = merged_df(df_part2_ques_df, df_part2_ans_df)\n",
        "merged_df_3 = merged_df(df_part3_ques_df, df_part3_ans_df)"
      ],
      "metadata": {
        "id": "18uLkEKeUQxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_ques_ans = pd.concat([merged_df_1, merged_df_2, merged_df_3], axis=0, ignore_index=True)"
      ],
      "metadata": {
        "id": "YNHm223uUSIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating .txt files to save question and answer pairs in good format**"
      ],
      "metadata": {
        "id": "PVloQpkUCh7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_directory = 'data_modified_new'\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "def format_output(data, part_number):\n",
        "    output = \"\"\n",
        "    output += f\"----------------- part_{part_number} ------------------\\n\"\n",
        "    for index, row in data.iterrows():\n",
        "        questions = row['question_txt']\n",
        "        answer = row['answer']\n",
        "        output += f\"- examiner: {questions}.\\n candidate: {answer}\\n\"\n",
        "\n",
        "    return output\n",
        "\n",
        "# Process merged_df_1\n",
        "for session, data in merged_df_1.groupby('session'):\n",
        "    formatted_output = format_output(data, 1)\n",
        "    with open(os.path.join(output_directory, f'session_{session}_part1.txt'), 'w') as file:\n",
        "        file.write(formatted_output)\n",
        "\n",
        "# Process merged_df_2\n",
        "for session, data in merged_df_2.groupby('session'):\n",
        "    formatted_output = format_output(data, 2)\n",
        "    with open(os.path.join(output_directory, f'session_{session}_part2.txt'), 'w') as file:\n",
        "        file.write(formatted_output)\n",
        "\n",
        "# Process merged_df_3\n",
        "for session, data in merged_df_3.groupby('session'):\n",
        "    formatted_output = format_output(data, 3)\n",
        "    with open(os.path.join(output_directory, f'session_{session}_part3.txt'), 'w') as file:\n",
        "        file.write(formatted_output)\n",
        "\n",
        "print(\"Processing complete. Files saved in the 'data_modified' directory.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFbPQWWfkWwM",
        "outputId": "b3130215-9d38-44b4-c673-4c02c9607133"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing complete. Files saved in the 'data_modified' directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_directory = 'combined_parts'\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "for session in merged_df_1['session'].unique():\n",
        "  part1_file = os.path.join('data_modified_new', f'session_{session}_part1.txt')\n",
        "  part2_file = os.path.join('data_modified_new', f'session_{session}_part2.txt')\n",
        "  part3_file = os.path.join('data_modified_new', f'session_{session}_part3.txt')\n",
        "  combined_file = os.path.join(output_directory, f'session_{session}.txt')\n",
        "\n",
        "  with open(combined_file, 'w') as outfile:\n",
        "    if os.path.exists(part1_file):\n",
        "      with open(part1_file, 'r') as infile:\n",
        "        outfile.write(infile.read())\n",
        "    if os.path.exists(part2_file):\n",
        "      with open(part2_file, 'r') as infile:\n",
        "        outfile.write(infile.read())\n",
        "    if os.path.exists(part3_file):\n",
        "      with open(part3_file, 'r') as infile:\n",
        "        outfile.write(infile.read())\n",
        "\n",
        "print(\"Combined files saved in the 'combined_parts' directory.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1E6HCPZk577",
        "outputId": "6a6947c9-33c3-41c7-a421-e334d9a6ac0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined files saved in the 'combined_parts' directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Downloading zip file**"
      ],
      "metadata": {
        "id": "vtC6e8ruCtF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "shutil.make_archive('/content/data_new_filtered', 'zip', '/content/data_new_filtered')\n",
        "\n",
        "files.download('data_new_filtered.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "nDZaksVoUf66",
        "outputId": "c1c51449-e27d-4314-a205-df96914691bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_09a021d0-bcd7-4ac0-89af-3ae70440d637\", \"data_new_filtered.zip\", 14982741)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "directory = '/content/combined_parts'"
      ],
      "metadata": {
        "id": "H1Dp8f2Hb7wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Woring with .txt file**"
      ],
      "metadata": {
        "id": "BRYCaFDYC00l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        session_number = ''.join(filter(str.isdigit, filename))\n",
        "        with open(os.path.join(directory, filename), 'r') as file:\n",
        "            content = file.read()\n",
        "            data.append({'session': session_number, 'content': content})\n",
        "\n",
        "df_txt = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "xHNaR1eKb3Sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_txt['session'] = df_txt['session'].astype('int64')"
      ],
      "metadata": {
        "id": "p-m1LxvGcBMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df_full = pd.merge(df_txt, df_parsed, on='session', how='left')"
      ],
      "metadata": {
        "id": "C4Eu8riScD9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_directory = 'data_new'\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "for index, row in merged_df_full.iterrows():\n",
        "    session_number = row['session']\n",
        "    content = row['transcript']\n",
        "    filename = f'session_{session_number}.txt'\n",
        "    filepath = os.path.join(output_directory, filename)\n",
        "    with open(filepath, 'w') as file:\n",
        "        file.write(content)\n",
        "\n",
        "print(\"Transcripts saved to individual files in the 'data_new' directory.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fz2gkQrcegXL",
        "outputId": "50400730-446c-448f-e0b1-d1b1604fef0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcripts saved to individual files in the 'data_new' directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df_full = merged_df_full.rename(columns={'content': 'transcript', 'raw_json': 'feedback_gpt', \"band_score\": \"band_score_gpt\"})\n",
        "merged_df_full.drop(columns=['parsed_json', 'feedback'], inplace=True)"
      ],
      "metadata": {
        "id": "nrAKElrVcQ3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df_full.drop(columns=['band_score_gpt'], inplace=True)"
      ],
      "metadata": {
        "id": "2sYmpRsEcoEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df_full.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "qxqHeXIBce0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = merged_df_full.groupby('session')['feedback_gpt'].apply(lambda x: x.str.len().idxmax())\n",
        "\n",
        "merged_df_full = merged_df_full.loc[idx]"
      ],
      "metadata": {
        "id": "UQmup2TddiG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df = df_txt[df_txt['content'].str.contains('part_1') &\n",
        "                     df_txt['content'].str.contains('part_2') &\n",
        "                     df_txt['content'].str.contains('part_3')]\n",
        "\n",
        "print(\"Number of rows before filtering:\", len(df_txt))\n",
        "print(\"Number of rows after filtering:\", len(filtered_df))\n",
        "\n",
        "df_txt = filtered_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRr5cG6qmo_E",
        "outputId": "0490e472-f685-4862-b0c4-bea451673868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows before filtering: 10943\n",
            "Number of rows after filtering: 9518\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_directory = 'data_new_filtered'\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "for index, row in df_txt.iterrows():\n",
        "    session_number = row['session']\n",
        "    content = row['content']\n",
        "    filename = f'session_{session_number}.txt'\n",
        "    filepath = os.path.join(output_directory, filename)\n",
        "    with open(filepath, 'w') as file:\n",
        "        file.write(content)\n",
        "\n",
        "print(\"Transcripts saved to individual files in the 'data_new_filtered' directory.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgQ75qK7ncou",
        "outputId": "c73cf1af-352f-4f72-b6c2-f97e536dfb43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcripts saved to individual files in the 'data_new_filtered' directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls data_2 | wc -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqFC8Mroc6WH",
        "outputId": "fa7e2eae-2dc0-4919-ce0f-779435fab953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dividing Dataset into parts**"
      ],
      "metadata": {
        "id": "fWgGoxjqC7eF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "data_folder = 'data_new_filtered'\n",
        "\n",
        "all_files = sorted(os.listdir(data_folder))\n",
        "\n",
        "files_per_folder = 2000\n",
        "\n",
        "for i in range(4):\n",
        "    start_index = i * files_per_folder\n",
        "    end_index = start_index + files_per_folder\n",
        "\n",
        "    if i == 3:\n",
        "        end_index = len(all_files)\n",
        "\n",
        "    new_folder = os.path.join(data_folder, f'folder_{i+1}')\n",
        "    os.makedirs(new_folder, exist_ok=True)\n",
        "\n",
        "    for file in all_files[start_index:end_index]:\n",
        "        shutil.move(os.path.join(data_folder, file), os.path.join(new_folder, file))\n",
        "\n",
        "print(\"Files have been successfully distributed into 4 folders.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p79YuUpIkxY_",
        "outputId": "6ce1e4ec-fb78-4f06-dd38-b29daadb52cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files divided into folders.\n"
          ]
        }
      ]
    }
  ]
}