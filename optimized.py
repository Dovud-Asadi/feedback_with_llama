import json
import string
from groq import Groq
import os
from concurrent.futures import ThreadPoolExecutor, as_completed

# Initialize the Groq API client with your API key.
client = Groq(
    api_key="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", #cs
)

# Directory containing the text files to be processed.
directory = '/home/ravshan/speaklish_data_prep/feedback_with_llama/unzipped_folder/data_new_filtered/folder_1'

# System message to guide the model's behavior.
system_message_modified = {
    "role": "system",
    "content": """
    Act as an IELTS Speaking examiner. ...nswered voice transcript>
    """
}

# Example content used for one-shot learning to guide the model's response.
content_user = """
------------------ part_1 ---------
- examiner: ...
"""

content_assis = """
 "Feedback:\n...
 """

# One-shot examples are included to help the model understand the task.
one_shot_examples = [
    {"role": "user", "content": content_user},
    {"role": "assistant", "content": content_assis}
]

# Output filename where the feedback responses will be saved.
output_filename = 'first_feedback_responses.json'

# Number of feedback responses to save at once (batch processing).
batch_size = 100

# Number of threads to use for concurrent processing (adjust based on your system and API limits).
max_workers = 5

def process_file(filename):
    """
    Processes a single file, sending the content to the API and returning the feedback.
    
    Args:
        filename (str): The name of the file to be processed.
    
    Returns:
        dict: A dictionary containing the session number and feedback generated by the model.
    """
    with open(os.path.join(directory, filename), 'r') as file:
        transcript = file.read()

    user_message = {
        "role": "user",
        "content": transcript
    }

    # Send a request to the Groq API to get feedback from the model.
    chat_completion = client.chat.completions.create(
        model="llama3-70b-8192",
        messages=[
            system_message_modified,
            *one_shot_examples,
            user_message
        ],
        max_tokens=1024,
        temperature=0.7
    )

    # Extract the model's response from the API result.
    model_response = chat_completion.choices[0].message.content

    # Extract session number from the filename (assuming digits represent the session).
    session_number = ''.join(filter(str.isdigit, filename))

    # Return the session number and feedback as a dictionary.
    return {
        "session": session_number,
        "feedback": model_response
    }

def save_feedback_to_file(feedback_list):
    """
    Saves the accumulated feedback to a JSON file.
    
    Args:
        feedback_list (list): A list of feedback dictionaries to save.
    """
    # Load existing data from the file if it exists.
    if os.path.exists(output_filename):
        with open(output_filename, 'r') as output_file:
            existing_data = json.load(output_file)
    else:
        existing_data = []

    # Append new feedback to the existing data.
    existing_data.extend(feedback_list)

    # Save the combined data back to the file.
    with open(output_filename, 'w') as output_file:
        json.dump(existing_data, output_file, indent=4)
    
    print(f"Batch of {len(feedback_list)} feedback responses added to {output_filename}.")

def main():
    """
    Main function that manages the processing of all files and batch saving.
    """
    all_feedback = []
    
    # List of all text files in the directory to process.
    files = [f for f in os.listdir(directory) if f.endswith(".txt")]

    # Use ThreadPoolExecutor to process files concurrently.
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit tasks to the executor and keep track of the futures (results).
        futures = {executor.submit(process_file, filename): filename for filename in files}

        # As each future completes, retrieve its result and append it to the feedback list.
        for future in as_completed(futures):
            try:
                feedback = future.result()
                all_feedback.append(feedback)
            except Exception as e:
                # Log any exceptions encountered during processing.
                print(f"Error processing {futures[future]}: {e}")

            # Save feedback in batches once the batch size is reached.
            if len(all_feedback) >= batch_size:
                save_feedback_to_file(all_feedback)
                all_feedback = []  # Clear the list after saving

    # Save any remaining feedback that didn't make up a full batch.
    if all_feedback:
        save_feedback_to_file(all_feedback)

if __name__ == "__main__":
    # Entry point of the script, starts the main function.
    main()
